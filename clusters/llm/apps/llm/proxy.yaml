apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-proxy
  namespace: llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-proxy
  template:
    metadata:
      labels:
        app: vllm-proxy
    spec:
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      containers:
        - name: proxy
          image: ghcr.io/sofianedjerbi/llama-domestique-proxy:87c91da3968325d8cc049e0b7308b643b83c6031 # {"$imagepolicy": "flux-system:proxy"}
          ports:
            - containerPort: 8080
          env:
            - name: VLLM_URL
              value: "http://vllm:8000"
            - name: RATE_LIMIT
              value: "10/minute"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-proxy
  namespace: llm
spec:
  selector:
    app: vllm-proxy
  ports:
    - port: 8080
      targetPort: 8080
